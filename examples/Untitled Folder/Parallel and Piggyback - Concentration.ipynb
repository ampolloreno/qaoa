{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Graph Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's very quick to generate a large number of graphs, so we should do this on the first node.\n",
    "# What is the probability of any 3-regular graph being isomorphic to another as v becomes large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "seed = 137\n",
    "np.random.seed(seed)\n",
    "# We might want more graphs, additionally we want to make sure that the graphs are saved in a way that it is easy\n",
    "# to recover the data, and make sure that new graphs are non-isomorphic.\n",
    "num_graphs_gen = 100\n",
    "graphs = []\n",
    "# We might want larger d.\n",
    "d = 3\n",
    "\n",
    "# We will scan over this parameter, it's not clear if it should scale multiplicatively, or additively.\n",
    "num_qubits = 4\n",
    "for _ in range(num_graphs_gen):\n",
    "    graphs.append(nx.generators.random_graphs.random_regular_graph(d, num_qubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate QAOA Landscape for each Graph Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the number of qubits increases, each circuit will become increasingly time consuming to simulate.\n",
    "# Additionally, the number of jobs we submit (and the quality of our estimates of the optimal parameters)\n",
    "# will scale with how well we discretize our grid. The feature sizes may shrink with n, so we may want our discretization\n",
    "# to be a function of n.\n",
    "\n",
    "# But there is also the question of how we want to break up the simulation. We could hand each node a graph, and ask it\n",
    "# to produce the landscape. This is nice because then no parent node has to clean up individual cost evaluations and \n",
    "# regather them. \n",
    "\n",
    "# Jiliac: 16nodes 8cores 90GB(per node)\n",
    "# Hexadec: 12nodes 16cores 120GB(per node)\n",
    "# NistQ: 10nodes 32cores 500GB(per node)\n",
    "# Dell: 16nodes 24cores 180GB(per node)\n",
    "\n",
    "# Conveniently the QASM Simulator already parallelizes across nodes, so I really just need to submit the jobs.\n",
    "# It will also parallelize after some number of qubits for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispatch_jobs.py\n",
    "from subprocess import call\n",
    "def write_graph(graph, attributes={}):\n",
    "    h = hashlib.md5()\n",
    "    arr = adjacency_matrix(graph).toarray()\n",
    "    h.update(arr)\n",
    "    hash_ = h.hexdigest()\n",
    "    filename = f'{hash_}.pkl'\n",
    "    try:\n",
    "        with open(filename, 'rb') as filehandle:\n",
    "            data = dill.load(filehandle)\n",
    "            print(\"Fetching existing file...\")\n",
    "    except FileNotFoundError:\n",
    "        data = {'graph': graph}\n",
    "    for k, v in attributes.items():\n",
    "        if data.get(k) is None:\n",
    "            data[k] = v\n",
    "        else:\n",
    "            print(f\"File {filename} already has attribute {k}, not overwriting.\")\n",
    "    with open(filename, 'wb') as filehandle:\n",
    "            data = dill.dump(data, filehandle)\n",
    "    return filename\n",
    "\n",
    "for graph in graphs:\n",
    "    filename = write_graph(graph)\n",
    "    cmd = f\"sbatch produce_landscape.py {filename}\"\n",
    "    call(cmd, shell=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce_landscape.py\n",
    "import sys\n",
    "def write_graph(graph, attributes={}):\n",
    "    h = hashlib.md5()\n",
    "    arr = adjacency_matrix(graph).toarray()\n",
    "    h.update(arr)\n",
    "    hash_ = h.hexdigest()\n",
    "    filename = f'{hash_}.pkl'\n",
    "    try:\n",
    "        with open(filename, 'rb') as filehandle:\n",
    "            data = dill.load(filehandle)\n",
    "            print(\"Fetching existing file...\")\n",
    "    except FileNotFoundError:\n",
    "        data = {'graph': graph}\n",
    "    for k, v in attributes.items():\n",
    "        if data.get(k) is None:\n",
    "            data[k] = v\n",
    "        else:\n",
    "            print(f\"File {filename} already has attribute {k}, not overwriting.\")\n",
    "    with open(filename, 'wb') as filehandle:\n",
    "            data = dill.dump(data, filehandle)\n",
    "    return filename\n",
    "\n",
    "def read_graph(filename):\n",
    "    with open(filename, 'rb') as filehandle:\n",
    "            data = dill.load(filehandle)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "from classical_optimization.qaoa_circuits import produce_gammas_betas, maxcut_qaoa_circuit\n",
    "from qiskit import Aer, execute\n",
    "from qiskit.providers.aer.extensions import snapshot_density_matrix\n",
    "\n",
    "def cost(density_matrix, num_qubits, weights):\n",
    "    rtn = 0\n",
    "    for edge, weight in weights.items():\n",
    "        rtn += .5 * weight * (1 - np.trace(Z(*edge, num_qubits).dot(density_matrix)))\n",
    "    return rtn\n",
    "\n",
    "def Z(i, j, num_qubits):\n",
    "    rtn = np.eye(1)\n",
    "    z = np.array([[1, 0], [0, -1]])\n",
    "    for k in range(num_qubits):\n",
    "        if k == i or k == j:\n",
    "            rtn = np.kron(rtn, z)\n",
    "        else:\n",
    "            rtn = np.kron(rtn, np.eye(2))\n",
    "    return rtn\n",
    "\n",
    "def weights(graph):\n",
    "    rtn = {}\n",
    "    for e in graph.edges:\n",
    "        try:\n",
    "            weight = graph.get_edge_data(e[0], e[1])['weight']\n",
    "        except KeyError:\n",
    "            weight = 1\n",
    "        rtn[e] = weight\n",
    "    return rtn\n",
    "\n",
    "discretization = 50\n",
    "max_gamma = 2*np.pi\n",
    "max_beta = np.pi\n",
    "gammas, betas = produce_gammas_betas(discretization, max_gamma, max_beta)\n",
    "\n",
    "filename = sys.argv[1]\n",
    "graph = read_graph(filename)['graph']\n",
    "num_qubits = len(graph.nodes)\n",
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "experiments = []\n",
    "\n",
    "for gamma in gammas:\n",
    "        for beta in betas:\n",
    "            circuit = maxcut_qaoa_circuit(gammas=[gamma], betas=[beta], p=1, num_qubits=num_qubits, weights=weights(graph), measure=False)\n",
    "            experiments.append(circuit)\n",
    "    job = execute(experiments, backend=simulator, backend_options=backend_options)    \n",
    "    outputs = [result.data.snapshots.density_matrix['output'][0]['value'] for result in job.result().results]\n",
    "    expectations = [cost(np.array(output)[:, :, 0], num_qubits=num_qubits, weights=weights(graph)) for output in outputs]\n",
    "    landscape = np.zeros((2*discretization, discretization))\n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for j, beta in enumerate(betas):\n",
    "            landscape[i][j] = expectations[i*len(betas) + j]\n",
    "    write_graph(graph, {f\"landscape_d{discretization}_b{max_beta}_g{max_gamma}\": landscape})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Optimal QAOA Parameters for Each Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
